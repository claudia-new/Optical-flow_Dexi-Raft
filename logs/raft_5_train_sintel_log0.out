nohup: ignoring input
Parameter Count: 42600909
Training with 293737 image pairs
/home/zexin/anaconda3/envs/raft/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
[   100,  0.0000056]     0.8170,     0.8978,     0.9188,     4.0238, 
[   200,  0.0000072]     0.8262,     0.9092,     0.9326,     3.0970, 
[   300,  0.0000088]     0.8266,     0.9144,     0.9371,     2.4641, 
[   400,  0.0000104]     0.8305,     0.9104,     0.9320,     2.9349, 
[   500,  0.0000120]     0.8199,     0.9046,     0.9272,     3.8404, 
[   600,  0.0000136]     0.8267,     0.9061,     0.9278,     3.3677, 
[   700,  0.0000152]     0.8154,     0.8986,     0.9218,     3.4327, 
[   800,  0.0000168]     0.8218,     0.9039,     0.9248,     4.0841, 
[   900,  0.0000184]     0.8308,     0.9099,     0.9301,     3.6215, 
[  1000,  0.0000200]     0.8553,     0.9302,     0.9481,     2.4317, 
[  1100,  0.0000216]     0.8241,     0.9046,     0.9268,     3.7181, 
[  1200,  0.0000232]     0.8287,     0.9103,     0.9321,     3.1865, 
[  1300,  0.0000248]     0.8398,     0.9183,     0.9380,     3.3149, 
[  1400,  0.0000264]     0.8326,     0.9112,     0.9317,     3.6844, 
[  1500,  0.0000280]     0.8327,     0.9141,     0.9367,     2.8743, 
[  1600,  0.0000296]     0.8329,     0.9111,     0.9324,     3.2081, 
[  1700,  0.0000312]     0.8226,     0.9081,     0.9304,     3.1410, 
[  1800,  0.0000328]     0.8273,     0.9095,     0.9308,     3.8681, 
[  1900,  0.0000344]     0.8372,     0.9195,     0.9417,     2.8767, 
[  2000,  0.0000360]     0.8441,     0.9226,     0.9429,     2.3169, 
[  2100,  0.0000376]     0.8382,     0.9182,     0.9387,     3.3080, 
[  2200,  0.0000392]     0.8318,     0.9132,     0.9334,     3.8027, 
[  2300,  0.0000408]     0.8384,     0.9176,     0.9377,     3.1713, 
[  2400,  0.0000424]     0.8249,     0.9083,     0.9301,     3.5899, 
[  2500,  0.0000440]     0.8353,     0.9128,     0.9339,     3.3721, 
[  2600,  0.0000456]     0.8080,     0.8956,     0.9187,     4.5495, 
[  2700,  0.0000472]     0.8179,     0.9022,     0.9271,     3.2433, 
[  2800,  0.0000488]     0.8471,     0.9194,     0.9370,     3.0372, 
[  2900,  0.0000504]     0.8267,     0.9138,     0.9365,     2.7583, 
[  3000,  0.0000520]     0.8341,     0.9128,     0.9324,     3.5930, 
[  3100,  0.0000536]     0.8324,     0.9154,     0.9361,     3.0733, 
[  3200,  0.0000551]     0.8392,     0.9175,     0.9380,     3.0233, 
[  3300,  0.0000567]     0.8404,     0.9193,     0.9392,     2.8689, 
[  3400,  0.0000583]     0.8356,     0.9119,     0.9323,     3.8083, 
[  3500,  0.0000599]     0.8394,     0.9172,     0.9382,     2.8170, 
[  3600,  0.0000615]     0.8213,     0.9019,     0.9242,     3.8269, 
[  3700,  0.0000631]     0.8312,     0.9155,     0.9380,     2.5531, 
[  3800,  0.0000647]     0.8177,     0.9013,     0.9257,     3.3637, 
[  3900,  0.0000663]     0.8306,     0.9120,     0.9328,     3.1166, 
[  4000,  0.0000679]     0.8370,     0.9127,     0.9324,     3.9626, 
[  4100,  0.0000695]     0.8350,     0.9145,     0.9352,     3.1986, 
[  4200,  0.0000711]     0.8461,     0.9241,     0.9450,     2.5701, 
[  4300,  0.0000727]     0.8317,     0.9120,     0.9328,     3.8540, 
[  4400,  0.0000743]     0.8281,     0.9087,     0.9305,     3.3968, 
[  4500,  0.0000759]     0.8197,     0.9047,     0.9265,     4.0174, 
[  4600,  0.0000775]     0.8265,     0.9075,     0.9287,     3.5356, 
[  4700,  0.0000791]     0.8352,     0.9150,     0.9354,     3.8785, 
[  4800,  0.0000807]     0.8273,     0.9098,     0.9306,     3.8174, 
[  4900,  0.0000823]     0.8270,     0.9094,     0.9315,     3.4431, 
[  5000,  0.0000839]     0.8318,     0.9142,     0.9369,     3.1138, 
Validation (clean) EPE: 1.215966, 1px: 0.906344, 3px: 0.959867, 5px: 0.972046
Validation (final) EPE: 2.057677, 1px: 0.863023, 3px: 0.930327, 5px: 0.948941
[  5100,  0.0000855]     0.8223,     0.9073,     0.9291,     3.7690, 
[  5200,  0.0000871]     0.8311,     0.9112,     0.9317,     3.5491, 
[  5300,  0.0000887]     0.8478,     0.9261,     0.9458,     2.2537, 
[  5400,  0.0000903]     0.8222,     0.9050,     0.9281,     3.0672, 
[  5500,  0.0000919]     0.8215,     0.9098,     0.9319,     3.3450, 
[  5600,  0.0000935]     0.8545,     0.9272,     0.9458,     2.4526, 
[  5700,  0.0000951]     0.8290,     0.9120,     0.9342,     2.9333, 
[  5800,  0.0000967]     0.8281,     0.9162,     0.9382,     2.9199, 
[  5900,  0.0000983]     0.8294,     0.9137,     0.9368,     2.6795, 
[  6000,  0.0000999]     0.8266,     0.9091,     0.9310,     3.4793, 
[  6100,  0.0000999]     0.8254,     0.9065,     0.9272,     4.3500, 
[  6200,  0.0000998]     0.8114,     0.8965,     0.9186,     5.2818, 
[  6300,  0.0000997]     0.8445,     0.9188,     0.9371,     3.1440, 
[  6400,  0.0000997]     0.8342,     0.9163,     0.9374,     3.3768, 
[  6500,  0.0000996]     0.8343,     0.9171,     0.9377,     2.6785, 
[  6600,  0.0000995]     0.8145,     0.9046,     0.9294,     2.9259, 
[  6700,  0.0000994]     0.8267,     0.9121,     0.9353,     2.7644, 
[  6800,  0.0000993]     0.8189,     0.9086,     0.9303,     3.5137, 
[  6900,  0.0000992]     0.8339,     0.9145,     0.9351,     3.1800, 
[  7000,  0.0000991]     0.8232,     0.9073,     0.9307,     3.1055, 
[  7100,  0.0000990]     0.8351,     0.9178,     0.9397,     2.4942, 
[  7200,  0.0000990]     0.8230,     0.9079,     0.9301,     3.2894, 
[  7300,  0.0000989]     0.8257,     0.9067,     0.9291,     3.2274, 
[  7400,  0.0000988]     0.8161,     0.9007,     0.9222,     4.1165, 
[  7500,  0.0000987]     0.8255,     0.9054,     0.9265,     5.5098, 
[  7600,  0.0000986]     0.8425,     0.9174,     0.9367,     3.1701, 
[  7700,  0.0000985]     0.8212,     0.9096,     0.9316,     3.1188, 
[  7800,  0.0000984]     0.8274,     0.9101,     0.9307,     3.7164, 
[  7900,  0.0000983]     0.8219,     0.9091,     0.9311,     3.5378, 
[  8000,  0.0000983]     0.8319,     0.9136,     0.9336,     3.2324, 
[  8100,  0.0000982]     0.8195,     0.9055,     0.9288,     3.1926, 
[  8200,  0.0000981]     0.8211,     0.9049,     0.9271,     3.7806, 
[  8300,  0.0000980]     0.8362,     0.9149,     0.9356,     2.9676, 
[  8400,  0.0000979]     0.8325,     0.9140,     0.9345,     3.5698, 
[  8500,  0.0000978]     0.8227,     0.9092,     0.9329,     2.8538, 
[  8600,  0.0000977]     0.8287,     0.9133,     0.9358,     3.4001, 
[  8700,  0.0000976]     0.8232,     0.9113,     0.9343,     3.5643, 
[  8800,  0.0000976]     0.8314,     0.9136,     0.9347,     3.2112, 
[  8900,  0.0000975]     0.8411,     0.9206,     0.9405,     3.0851, 
[  9000,  0.0000974]     0.8388,     0.9129,     0.9328,     3.6015, 
[  9100,  0.0000973]     0.8381,     0.9135,     0.9336,     2.9283, 
[  9200,  0.0000972]     0.8266,     0.9103,     0.9317,     3.6615, 
[  9300,  0.0000971]     0.8401,     0.9199,     0.9407,     2.4571, 
[  9400,  0.0000970]     0.8347,     0.9141,     0.9355,     2.8396, 
[  9500,  0.0000969]     0.8312,     0.9098,     0.9304,     3.2192, 
[  9600,  0.0000968]     0.8459,     0.9243,     0.9442,     2.5043, 
[  9700,  0.0000968]     0.8385,     0.9126,     0.9318,     3.5478, 
[  9800,  0.0000967]     0.8441,     0.9187,     0.9380,     2.7234, 
[  9900,  0.0000966]     0.8335,     0.9151,     0.9373,     2.8707, 
[ 10000,  0.0000965]     0.8332,     0.9127,     0.9332,     3.0573, 
Validation (clean) EPE: 1.168128, 1px: 0.907116, 3px: 0.960267, 5px: 0.972397
Validation (final) EPE: 1.964704, 1px: 0.866464, 3px: 0.932544, 5px: 0.951524
[ 10100,  0.0000964]     0.8523,     0.9264,     0.9443,     3.1135, 
[ 10200,  0.0000963]     0.8230,     0.9049,     0.9273,     3.4916, 
[ 10300,  0.0000962]     0.8106,     0.8975,     0.9208,     4.0369, 
[ 10400,  0.0000961]     0.8285,     0.9072,     0.9299,     3.2127, 
[ 10500,  0.0000961]     0.8408,     0.9181,     0.9382,     2.9475, 
[ 10600,  0.0000960]     0.8349,     0.9151,     0.9372,     2.7741, 
[ 10700,  0.0000959]     0.8237,     0.9100,     0.9321,     3.3527, 
[ 10800,  0.0000958]     0.8282,     0.9096,     0.9314,     3.0737, 
[ 10900,  0.0000957]     0.8328,     0.9178,     0.9384,     3.2888, 
[ 11000,  0.0000956]     0.8295,     0.9104,     0.9324,     3.2908, 
[ 11100,  0.0000955]     0.8375,     0.9191,     0.9414,     2.8501, 
[ 11200,  0.0000954]     0.8365,     0.9146,     0.9357,     2.9049, 
[ 11300,  0.0000954]     0.8292,     0.9088,     0.9305,     3.6248, 
[ 11400,  0.0000953]     0.8328,     0.9178,     0.9397,     2.9192, 
[ 11500,  0.0000952]     0.8381,     0.9191,     0.9397,     2.9370, 
[ 11600,  0.0000951]     0.8417,     0.9222,     0.9420,     2.7129, 
[ 11700,  0.0000950]     0.8403,     0.9161,     0.9355,     4.2526, 
Traceback (most recent call last):
  File "train.py", line 250, in <module>
    train(args)
  File "train.py", line 166, in train
    for i_batch, data_blob in enumerate(train_loader):
  File "/home/zexin/anaconda3/envs/raft/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 363, in __next__
    data = self._next_data()
  File "/home/zexin/anaconda3/envs/raft/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 971, in _next_data
    return self._process_data(data)
  File "/home/zexin/anaconda3/envs/raft/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 1014, in _process_data
    data.reraise()
  File "/home/zexin/anaconda3/envs/raft/lib/python3.7/site-packages/torch/_utils.py", line 395, in reraise
    raise self.exc_type(msg)
OSError: Caught OSError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/zexin/anaconda3/envs/raft/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py", line 185, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/zexin/anaconda3/envs/raft/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/zexin/anaconda3/envs/raft/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/zexin/anaconda3/envs/raft/lib/python3.7/site-packages/torch/utils/data/dataset.py", line 207, in __getitem__
    return self.datasets[dataset_idx][sample_idx]
  File "/home/zexin/anaconda3/envs/raft/lib/python3.7/site-packages/torch/utils/data/dataset.py", line 207, in __getitem__
    return self.datasets[dataset_idx][sample_idx]
  File "/home/zexin/anaconda3/envs/raft/lib/python3.7/site-packages/torch/utils/data/dataset.py", line 207, in __getitem__
    return self.datasets[dataset_idx][sample_idx]
  [Previous line repeated 1 more time]
  File "core/datasets.py", line 58, in __getitem__
    flow = frame_utils.read_gen(self.flow_list[index])
  File "core/utils/frame_utils.py", line 130, in read_gen
    return readFlow(file_name).astype(np.float32)
  File "core/utils/frame_utils.py", line 19, in readFlow
    with open(fn, 'rb') as f:
OSError: [Errno 112] Host is down: '/mnt/dst_datasets/optical_flow/Sintel/training/flow/market_6/frame_0013.flo'

